<!DOCTYPE html>
<html>
  <head>
    <title>MediaPipe Pose/Stroke Analysis</title>

    <!-- post detection model -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose"
      onload="window.ReactNativeWebView.postMessage('Pose loaded');"
      onerror="window.ReactNativeWebView.postMessage('Pose failed to load');">
    </script>
    <!-- drawing landmarks and connections -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils"
      onload="window.ReactNativeWebView.postMessage('Drawing utils loaded');"
      onerror="window.ReactNativeWebView.postMessage('Drawing utils failed to load');">
    </script>
    <!-- camera input -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils"
      onload="window.ReactNativeWebView.postMessage('Camera utils loaded');"
      onerror="window.ReactNativeWebView.postMessage('Camera utils failed to load');">
    </script>
    <style>
      html, body {
        height: 100%;
        margin: 0;
        padding: 0;
        /* allows for scrolling */
        overflow: auto;
        /* smoother scrolling in iOS */
        -webkit-overflow-scrolling: touch; 
      }
      video {
        width: 100%;
        height: auto;
      }
      canvas {
        width: 100%;
        height: auto;
        display: block;
      }
    </style>
  </head>
  <body>
    <h1 style="color: red; text-align: center;">HTML is Loaded and Visible!</h1>

    <!-- video component that loads and plays the video passed from React Native Expo -->
    <video id="input_vid" controls playsinline
      onerror="window.ReactNativeWebView.postMessage('Video error: ' + event.target.error.code)"
      onloadeddata="window.ReactNativeWebView.postMessage('Video loaded')"
      onplaying="window.ReactNativeWebView.postMessage('Video playing')">
    </video>
    
    <!-- draws pose landmarks over video -->
    <canvas id="output_canvas"></canvas>

    <script>
      let vid = document.getElementById("input_vid");
      let canvas = document.getElementById("output_canvas");
      let canvas_context = canvas.getContext('2d');

      let pose;
      //tracks results (individual frames)
      let poseFrames = [];

      //function that normalizes landmarks relative to the hips
      function normalize(landmarks) {
        //grabs hip landmarks from MediaPipe
        const leftHip = landmarks[23];
        const rightHip = landmarks[24];

        //calculates the midpoint between the hips (in both the x and y direction)
        //acts as the new origin for the body
        const midpointX = (leftHip.x + rightHip.x) / 2;
        const midpointY = (leftHip.y + rightHip.y) / 2;

        //calculates distance between the hips
        //allows for size of individual to no longer matter; allows for focus on pose/shape
        const scale = Math.hypot(leftHip.x - rightHip.x, leftHip.y - rightHip.y);

        return landmarks.map(landmark => ({
          //subtract midpoint to move origin to the hips
          //divide by scale to normalize for body size
          x: (landmark.x - midpointX) / scale,
          y: (landmark.y - midpointY) / scale,
          //z and visibility are unchanged
          z: landmark.z,
          visibility: landmark.visibility
        }));
      }

      //function that calculates the angle between three points
      function calculateAngle(a, b, c) {
        //create vectors
        const u = {x: a.x - b.x, y: a.y - b.y};
        const v = {x: c.x - b.x, y: c.y - b.y};

        //cos(theta) = dot product / product of magnitudes

        //dot product
        const dot = (u.x * v.x) + (u.y * v.y);

        //calculating the magnitudes
        const mag_u = Math.hypot(u.x, u.y);
        const mag_v = Math.hypot(v.x, v.y);

        //take arccos for the angle
        //returns angle in degrees
        return Math.acos(dot / (mag_u * mag_v)) * (180 / Math.PI);
      }

      //function for phase detection
      function detectPhase(landmarks) {
        const wrist = landmarks[16]; // right wrist
        const elbow = landmarks[14]; // right elbow
        const shoulder = landmarks[12]; //right shoulder
        const hip = landmarks[24]; //right hip

        if (wrist.x < hip.x) {
          return 'Preparation';
        }
        else if (elbow.y > shoulder.y && wrist.x > elbow.x) {
          return 'Acceleration';
        }
        else if (Math.abs(wrist.x - shoulder.x) < 0.05) {
          return 'Contact';
        }
        else if (wrist.x > shoulder.x) {
          return 'Follow-through';
        }
        return 'Unknown';
      }

      window.handleVideoUri = function(video_uri) { //function takes uri sent by React Native
        window.ReactNativeWebView.postMessage("Handling video URI: " + video_uri);
        vid.src = video_uri;

        //creates MediaPipe pose instance
        pose = new window.Pose({
          locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}`
        });
        pose.setOptions({
          modelComplexity: 1,
          smoothLandmarks: true,
          enableSegmentation: false,
          minDetectionConfidence: 0.5,
          minTrackingConfidence: 0.5
        });

        //draws landmarks and connectors
        pose.onResults(results => {
          //sets canvas size
          if (canvas.width !== vid.videoWidth || canvas.height !== vid.videoHeight) {
            canvas.width = vid.videoWidth;
            canvas.height = vid.videoHeight;
          }

          //saves canvas state
          canvas_context.save();
          //clears any previous drawings
          canvas_context.clearRect(0, 0, canvas.width, canvas.height);
          //draws the current video frame with results from MediaPipe (results.image) onto the canvas
          canvas_context.drawImage(results.image, 0, 0, canvas.width, canvas.height);

          if (results.poseLandmarks) {

            //calls functions created above
            const norm = normalize(results.poseLandmarks);
            const phase = detectPhase(norm);

            poseFrames.push({
              timestamp: vid.currentTime, phase, //records timestamp and phase
              landmarks: norm //adds normalized landmark

              //landmarks: JSON.parse(JSON.stringify(results.poseLandmarks)), // deep copy of landmark
            });

            //takes landmarks pairs (either through POSE_CONNECTIONS or manually set up pairs)
            const POSE_CONNECTIONS = window.Pose?.POSE_CONNECTIONS || [
              [0,1],[1,2],[2,3],[3,7],
              [0,4],[4,5],[5,6],[6,8],
              [9,10],[11,12],
              [11,13],[13,15],[15,17],[15,19],[15,21],
              [17,19],[12,14],[14,16],[16,18],[16,20],[16,22],
              [18,20],[23,24],
              [11,23], [12,24],
              [23,25],[24,26],[25,27],[26,28],
              [27,29],[28,30],[29,31],[30,32]
            ];

            //draws the connectors (lines)
            drawConnectors(canvas_context, results.poseLandmarks, POSE_CONNECTIONS, {
              color: '#FF0000',
              lineWidth: 2,
            });
            //draws the landmarks (points)
            drawLandmarks(canvas_context, results.poseLandmarks, {
              color: '#FF0000',
              lineWidth: 1,
              circleRadius: 3,
            });

            //display phase classification on canvas
            canvas_context.font = "bold 100px Arial";
            canvas_context.fillStyle = 'yellow';
            canvas_context.fillText(`Phase: ${phase}`, 20, 50);

            //sends phase to React Native
            window.ReactNativeWebView.postMessage(JSON.stringify({
              time: vid.currentTime,
              phase: phase
            }));
          }
          //restores the canvas state after drawing
          canvas_context.restore();
        });

        // sends video frames to pose after video plays
        vid.onplay = () => {
          window.ReactNativeWebView.postMessage("Video playing, starting pose processing.");
          //starts loop by calling onVideoFrame function each frame
          requestAnimationFrame(onVideoFrame);
        };

        function onVideoFrame() {
          //if video is ready and playing
          if (vid.readyState >= 2 && !vid.paused && !vid.ended) {
            //sends video frame to pose model for processing
            pose.send({image: vid});
          }
          //continues loop by requesting next frame
          requestAnimationFrame(onVideoFrame);
        }
      };

      window.onload = function() {
        window.ReactNativeWebView.postMessage('pageReady');
        if (window.Pose) {
          window.ReactNativeWebView.postMessage('Pose is available');
        } else {
          window.ReactNativeWebView.postMessage('Pose is not available');
        }
      };

    </script>
  </body>
</html>
